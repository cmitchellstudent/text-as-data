{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 1-5-2 First Feature Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequencies\n",
    "\n",
    "The basis for comparing texts begins with term frequencies. From there you can examine relative frequencies and topics, to name two things which we will be doing shortly. As you can imagine, we have to start from our ground truth and then decide how we are going to abstract in order to facilitate the kinds of comparisons we want to make. If we are interested in stylistics and/or attribution, then we will want to pay attention to the *function words* and often punctuation which often contain author signals. If we are interested in topics, then we will want to pay attention to *lexical words* (and throw away the function words).\n",
    "\n",
    "To do any of these things, we need to establish the term frequency for each token in each text for all our texts. As you can see from that statement, this involves several steps:\n",
    "\n",
    "1. Determine what we are going to include as tokens: words, words+punctuation, words-stopwords, etc.\n",
    "2. Count those tokens in a text\n",
    "3. Compile the tokens and their frequencies across our corpus\n",
    "\n",
    "If you're thinking \"That's a lot of work,\" you are correct but there is a process, and it looks like this ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set plt parameters\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"mdg\"]\n",
    "\n",
    "strings = []\n",
    "for i in files:\n",
    "    # Create the path to the file\n",
    "    the_file = \"../data/1924/texts/\"+i+\".txt\"\n",
    "    # Read the file to a string\n",
    "    the_string =  open(the_file, 'r').read()\n",
    "    # Add the string to a list of strings\n",
    "    strings.append(the_string)\n",
    "\n",
    "print(len(strings), strings[8][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"labels\":files, \"text\":strings})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.text.tolist()\n",
    "print(texts[8][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization(s)\n",
    "\n",
    "Before we can count words and establish frequencies, we need to settle upon what we are going to consider words, which means determining our method of tokenizing our strings of characters into lists of tokens.\n",
    "\n",
    "- The first tokenizer is regex that I have long used in order to keep contractions as single words, but it throws away all other forms of punctuation.\n",
    "- The NLTK's `word_tokenize()` function is based on a TreebankWordTokenizer: basically it tokenizes text like in the Penn Treebank, which means apostrophes break contractions into their distinct parts â€” e.g., `I'm` becomes `I` + `'m`. Whereas `wordpunct_tokenize()` is a regex that breaks the apostrophes of contractions into their own tokens.\n",
    "- SciKit Learn's tokenization comes up the leanest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGEX\n",
    "regex = [word for word in re.sub(\"[^a-zA-Z']\",\" \", texts[8]).lower().split()]\n",
    "\n",
    "# NLTK\n",
    "w_tokens = [word.lower() for word in word_tokenize(texts[8])]\n",
    "wp_tokens = [word.lower() for word in wordpunct_tokenize(texts[8])]\n",
    "\n",
    "# SciKit-Learn\n",
    "vectorizer = CountVectorizer( lowercase = True ) # We are vectorizing\n",
    "x = vectorizer.fit_transform([texts[8]])         # the same text as above\n",
    "sk_count = np.sum(x.toarray(), axis = 1)            # then summing the freq count\n",
    "\n",
    "# Print to Compare\n",
    "print(f\"regex:       {len(regex)}\")\n",
    "print(f\"nltk words:  {len(w_tokens)}\")\n",
    "print(f\"nltk wpunct: {len(wp_tokens)}\")\n",
    "print(f\"scikit:      {sk_count[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare vocabulary sizes:\n",
    "print(f\"METHOD : TOKEN SET\")\n",
    "print(f\"regex  :  {len(set(regex))}\")\n",
    "print(f\"NLTK   :  {len(set(w_tokens))}\")\n",
    "print(f\"SciKit :  {x.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = set(w_tokens) - set(vectorizer.get_feature_names_out())\n",
    "print(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These experiments reveal the strengths and weaknesses of SciKit-Learn's built-in tokenizer. We will explore alternate tokenizers later, for now, please be aware that if you run `CountVectorizer` unadorned, it has the following defaults:\n",
    "\n",
    "- lowercase everything, \n",
    "- get rid of all punctuation, \n",
    "- make a word out of anything more than two characters long, \n",
    "- split contractions, and \n",
    "- no stopwords.\n",
    "\n",
    "The tokenizer is not without its problems: while it breaks contractions at the apostrophe, like NLTK, it then throws away anything less than two letters, which means `I'm` disappears entirely. And pity the indefinite article *a(n)*, which is pitched while the definite article *the* remains. (More on this later, but you should know that the documentation for the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is quite good.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going with the defaults, \n",
    "# so no options/arguments are being passed:\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the model to the data \n",
    "# vecs = vectorizer.fit(texts)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# see how many features we have\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our nine observations, we have over seven thousand features!\n",
    "\n",
    "The easiest way to \"see\" this is to convert the array to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert:\n",
    "df = pd.DataFrame(X.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "\n",
    "# See what this looks like:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, we can save to a CSV file and look at this in other apps\n",
    "# df.to_csv(\"../data/mdg_texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_min = CountVectorizer(min_df = 2)\n",
    "X2 = vectorizer_min.fit_transform(texts)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(X2.toarray(), \n",
    "                   columns = vectorizer_min.get_feature_names_out())\n",
    "\n",
    "df2.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"label\"] = files\n",
    "df2.set_index(\"label\", inplace=True)\n",
    "df2.head(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
